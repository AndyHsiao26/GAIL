{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.distributions as dist\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "tf.set_random_seed(2019)\n",
    "np.random.seed(2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create Environments</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "num_envs = 16\n",
    "env_name = \"Pendulum-v0\"\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Neural Network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic:\n",
    "    def __init__(self, sess, obs, acs, hidden_size, name, trainable, init_std=1.0):\n",
    "        self.sess = sess\n",
    "        self.obs = obs\n",
    "        self.acs = acs\n",
    "        self.hidden_size = hidden_size\n",
    "        self.name = name\n",
    "        self.trainable = trainable\n",
    "        self.init_std = init_std\n",
    "\n",
    "        self.num_ac = self.acs.get_shape().as_list()[-1]\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            self._build_network()\n",
    "\n",
    "    def _build_network(self):\n",
    "        with tf.variable_scope('critic'):\n",
    "            c_h1 = layers.fully_connected(self.obs, self.hidden_size, trainable=self.trainable)\n",
    "            c_out = layers.fully_connected(c_h1, 1, activation_fn=None, trainable=self.trainable)\n",
    "\n",
    "        with tf.variable_scope('actor'):\n",
    "            a_h1 = layers.fully_connected(self.obs, self.hidden_size, trainable=self.trainable)\n",
    "            a_out = layers.fully_connected(a_h1, self.num_ac, activation_fn=None, trainable=self.trainable)\n",
    "\n",
    "            log_std = tf.get_variable('log_std', [1, self.num_ac], dtype=tf.float32,\n",
    "                                      initializer=tf.constant_initializer(self.init_std),\n",
    "                                      trainable=self.trainable)\n",
    "\n",
    "        std = tf.exp(log_std)\n",
    "        a_dist = dist.Normal(a_out, std)\n",
    "        self.log_prob = a_dist.log_prob(self.acs)\n",
    "        self.entropy = tf.reduce_mean(a_dist.entropy())\n",
    "\n",
    "        self.value = tf.identity(c_out)\n",
    "        self.action = a_dist.sample()\n",
    "\n",
    "    def params(self):\n",
    "        return tf.global_variables(self.name).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>GAE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Proximal Policy Optimization Algorithm</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, sess, ob_shape, ac_shape, lr, hidden_size, eps=0.2, v_coeff=0.5, ent_coeff=0.01):\n",
    "        self.sess = sess\n",
    "        self.ob_shape = ob_shape\n",
    "        self.ac_shape = ac_shape\n",
    "        self.lr = lr\n",
    "        self.hidden_size = hidden_size\n",
    "        self.eps = eps\n",
    "        self.v_coeff = v_coeff\n",
    "        self.ent_coeff = ent_coeff\n",
    "\n",
    "        self._create_ppo_graph()\n",
    "\n",
    "    def _create_ppo_graph(self):\n",
    "        self.obs = tf.placeholder(dtype=tf.float32, shape=[None] + self.ob_shape, name='observation')\n",
    "        self.acs = tf.placeholder(dtype=tf.float32, shape=[None] + self.ac_shape, name='action')\n",
    "        self.returns = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "        self.advs = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "\n",
    "        self.pi = ActorCritic(self.sess, self.obs, self.acs, self.hidden_size, 'new_pi', trainable=True)\n",
    "        self.old_pi = ActorCritic(self.sess, self.obs, self.acs, self.hidden_size, 'old_pi', trainable=False)\n",
    "\n",
    "        self.pi_param = self.pi.params()\n",
    "        self.old_pi_param = self.old_pi.params()\n",
    "\n",
    "        with tf.name_scope('update_old_policy'):\n",
    "            self.oldpi_update = [oldp.assign(p) for p, oldp in zip(self.pi_param, self.old_pi_param)]\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            ratio = tf.exp(self.pi.log_prob - self.old_pi.log_prob)\n",
    "            surr = ratio * self.advs\n",
    "            self.actor_loss = tf.reduce_mean(\n",
    "                tf.minimum(surr, tf.clip_by_value(ratio, 1 - self.eps, 1 + self.eps) * self.advs))\n",
    "            self.critic_loss = tf.reduce_mean(tf.square(self.returns - self.pi.value))\n",
    "\n",
    "            self.loss = (- self.actor_loss - self.ent_coeff * tf.reduce_mean(self.pi.entropy)\n",
    "                         + self.v_coeff * self.critic_loss)\n",
    "\n",
    "            with tf.variable_scope('train_op'):\n",
    "                grads = tf.gradients(self.loss, self.pi_param)\n",
    "                self.grads = list(zip(grads, self.pi_param))\n",
    "                self.train_op = tf.train.AdamOptimizer(self.lr).apply_gradients(self.grads)\n",
    "                                                                                #global_step=self.global_step)\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        return self.sess.run(self.pi.action, feed_dict={self.obs: obs})\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        return self.sess.run(self.pi.value, feed_dict={self.obs: obs})\n",
    "\n",
    "    def assign_old_pi(self):\n",
    "        self.sess.run(self.oldpi_update)\n",
    "\n",
    "    def update(self, obs, acs, returns, advs):\n",
    "        feed_dict = {self.obs: obs,\n",
    "                     self.acs: acs,\n",
    "                     self.returns: returns,\n",
    "                     self.advs: advs\n",
    "                     }\n",
    "\n",
    "        self.sess.run(self.train_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(mini_batch_size, obs, acs, returns, advantage):\n",
    "    batch_size = obs.shape[0]\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield (obs[rand_ids, :], acs[rand_ids, :],\n",
    "               returns[rand_ids, :], advantage[rand_ids, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Testing Function</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "def test_env(model, vis=False):\n",
    "    ob = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        if vis:\n",
    "            env.render()\n",
    "        ac = model.get_action([ob])[0]\n",
    "        next_ob, reward, done, _ = env.step(ac)\n",
    "        ob = next_ob\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hyperparameters</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "lr = 3e-4\n",
    "num_steps = 20\n",
    "mini_batch_size = 5\n",
    "ppo_epochs = 4\n",
    "threshold_reward = -200\n",
    "\n",
    "max_frames = 15000\n",
    "frame_idx  = 0\n",
    "test_rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ob_shape = list(envs.observation_space.shape)\n",
    "ac_shape = list(envs.action_space.shape)\n",
    "\n",
    "ob = envs.reset()\n",
    "early_stop = False\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "ppo = PPO(sess, ob_shape, ac_shape, lr, hidden_size)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "while frame_idx < max_frames and not early_stop:\n",
    "\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    obs = []\n",
    "    acs = []\n",
    "    rewards = []\n",
    "    masks = []\n",
    "    entropy = 0\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "\n",
    "        ac = ppo.get_action(ob)\n",
    "        next_ob, reward, done, _ = envs.step(ac)\n",
    "\n",
    "        value = ppo.get_value(ob)\n",
    "        values.append(value)\n",
    "        rewards.append(reward[:, np.newaxis])\n",
    "        masks.append((1-done)[:, np.newaxis])\n",
    "\n",
    "        obs.append(ob)\n",
    "        acs.append(ac)\n",
    "\n",
    "        ob = next_ob\n",
    "        frame_idx += 1\n",
    "\n",
    "        if frame_idx % 1000 == 0:\n",
    "            test_reward = np.mean([test_env(ppo) for _ in range(10)])\n",
    "            test_rewards.append(test_reward)\n",
    "            plot(frame_idx, test_rewards)\n",
    "            if test_reward > threshold_reward: early_stop = True\n",
    "\n",
    "    next_value = ppo.get_value(next_ob)\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "    returns = np.concatenate(returns)\n",
    "    values = np.concatenate(values)\n",
    "    obs = np.concatenate(obs)\n",
    "    acs = np.concatenate(acs)\n",
    "    advantages = returns - values\n",
    "\n",
    "    ppo.assign_old_pi()\n",
    "    for _ in range(ppo_epochs):\n",
    "        for ob_batch, ac_batch, return_batch, adv_batch in ppo_iter(mini_batch_size, obs, acs, returns, advantages):\n",
    "            ppo.update(ob_batch, ac_batch, return_batch, adv_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 reward: -133.7895303324179\n",
      "episode: 1 reward: -2.7525447033934696\n",
      "episode: 2 reward: -4.765489230356579\n",
      "episode: 3 reward: -118.18381635628376\n",
      "episode: 4 reward: -130.7147530718764\n",
      "episode: 5 reward: -122.73907341703023\n",
      "episode: 6 reward: -249.3751087063931\n",
      "episode: 7 reward: -243.367876320551\n",
      "episode: 8 reward: -248.3376759323275\n",
      "episode: 9 reward: -2.872011625613069\n",
      "episode: 10 reward: -349.1296163707549\n",
      "episode: 11 reward: -125.95388905967093\n",
      "episode: 12 reward: -121.62616271804646\n",
      "episode: 13 reward: -353.5198615287514\n",
      "episode: 14 reward: -351.1364958523882\n",
      "episode: 15 reward: -458.9897262729003\n",
      "episode: 16 reward: -349.51297055200524\n",
      "episode: 17 reward: -1.6285947949979067\n",
      "episode: 18 reward: -130.04936415365904\n",
      "episode: 19 reward: -131.53042590924355\n",
      "episode: 20 reward: -361.3807169776947\n",
      "episode: 21 reward: -126.08535224696546\n",
      "episode: 22 reward: -131.04334581289254\n",
      "episode: 23 reward: -127.82934427248324\n",
      "episode: 24 reward: -131.76699444891761\n",
      "episode: 25 reward: -2.3151992827602177\n",
      "episode: 26 reward: -119.57999541631445\n",
      "episode: 27 reward: -248.49063883105862\n",
      "episode: 28 reward: -135.02903396849555\n",
      "episode: 29 reward: -128.0468206219936\n",
      "episode: 30 reward: -245.56774888017702\n",
      "episode: 31 reward: -255.5986427787225\n",
      "episode: 32 reward: -125.35337135828868\n",
      "episode: 33 reward: -257.0293432890188\n",
      "episode: 34 reward: -351.14369439346956\n",
      "episode: 35 reward: -240.6344546520779\n",
      "episode: 36 reward: -260.6225607453666\n",
      "episode: 37 reward: -230.1771848541606\n",
      "episode: 38 reward: -356.87397639564875\n",
      "episode: 39 reward: -237.17191732546758\n",
      "episode: 40 reward: -124.66558804387172\n",
      "episode: 41 reward: -245.11490928096444\n",
      "episode: 42 reward: -252.07300280160993\n",
      "episode: 43 reward: -127.4867522535661\n",
      "episode: 44 reward: -128.12834177613894\n",
      "episode: 45 reward: -124.3271277290704\n",
      "episode: 46 reward: -116.88368331542972\n",
      "episode: 47 reward: -118.88244053861045\n",
      "episode: 48 reward: -2.528819678817669\n",
      "episode: 49 reward: -254.69557044770235\n",
      "episode: 50 reward: -125.29943150063092\n",
      "episode: 51 reward: -120.31765857380532\n",
      "episode: 52 reward: -130.7225978387168\n",
      "episode: 53 reward: -119.05470919546053\n",
      "episode: 54 reward: -132.6947314733738\n",
      "episode: 55 reward: -249.21529895868957\n",
      "episode: 56 reward: -404.03959390110407\n",
      "episode: 57 reward: -126.22770779976545\n",
      "episode: 58 reward: -5.3357623627272295\n",
      "episode: 59 reward: -368.53436736800666\n",
      "episode: 60 reward: -262.6512290885548\n",
      "episode: 61 reward: -126.69016391712923\n",
      "episode: 62 reward: -248.87612699842478\n",
      "episode: 63 reward: -121.83974443011974\n",
      "episode: 64 reward: -127.94252576103601\n",
      "episode: 65 reward: -399.7901335304546\n",
      "episode: 66 reward: -255.308574957811\n",
      "episode: 67 reward: -250.76482131854448\n",
      "episode: 68 reward: -133.82812695436735\n",
      "episode: 69 reward: -134.8876428821837\n",
      "episode: 70 reward: -250.60625015472564\n",
      "episode: 71 reward: -130.38718294739348\n",
      "episode: 72 reward: -2.5326750181440927\n",
      "episode: 73 reward: -132.41009407206317\n",
      "episode: 74 reward: -237.64255486394765\n",
      "episode: 75 reward: -374.4243533835124\n",
      "episode: 76 reward: -335.9329502943734\n",
      "episode: 77 reward: -119.37803963124925\n",
      "episode: 78 reward: -124.17922462446784\n",
      "episode: 79 reward: -126.67692887216353\n",
      "episode: 80 reward: -255.2784305401558\n",
      "episode: 81 reward: -350.4978428294286\n",
      "episode: 82 reward: -343.1310662842543\n",
      "episode: 83 reward: -580.1753100609569\n",
      "episode: 84 reward: -125.67065354794893\n",
      "episode: 85 reward: -129.4061074397663\n",
      "episode: 86 reward: -364.2821311512896\n",
      "episode: 87 reward: -132.1604336725005\n",
      "episode: 88 reward: -384.1495232762297\n",
      "episode: 89 reward: -132.5509990562993\n",
      "episode: 90 reward: -250.12368121599224\n",
      "episode: 91 reward: -370.13815907124507\n",
      "episode: 92 reward: -119.95116762121393\n",
      "episode: 93 reward: -135.36805501363983\n",
      "episode: 94 reward: -128.4072739834304\n",
      "episode: 95 reward: -120.80140019002926\n",
      "episode: 96 reward: -126.78380545987591\n",
      "episode: 97 reward: -365.9164197741052\n",
      "episode: 98 reward: -128.48976923099943\n",
      "episode: 99 reward: -369.94403076594847\n",
      "episode: 100 reward: -249.81148256671617\n",
      "episode: 101 reward: -381.05115432809396\n",
      "episode: 102 reward: -2.4039919667063723\n",
      "episode: 103 reward: -483.9980038247723\n",
      "episode: 104 reward: -129.98709650242685\n",
      "episode: 105 reward: -250.31682281298924\n",
      "episode: 106 reward: -129.41257614584458\n",
      "episode: 107 reward: -251.65035889411232\n",
      "episode: 108 reward: -2.0699581254745154\n",
      "episode: 109 reward: -132.10739379112135\n",
      "episode: 110 reward: -131.21868976579063\n",
      "episode: 111 reward: -259.66867912366223\n",
      "episode: 112 reward: -243.1028960881596\n",
      "episode: 113 reward: -130.34522129042577\n",
      "episode: 114 reward: -126.61518681889964\n",
      "episode: 115 reward: -134.6371557402829\n",
      "episode: 116 reward: -357.6211542114248\n",
      "episode: 117 reward: -130.9817848978754\n",
      "episode: 118 reward: -369.04278511974826\n",
      "episode: 119 reward: -127.52715118079543\n",
      "episode: 120 reward: -229.90416235946495\n",
      "episode: 121 reward: -1.8168288952696219\n",
      "episode: 122 reward: -2.3476288780203243\n",
      "episode: 123 reward: -431.94889425643123\n",
      "episode: 124 reward: -126.43984835210996\n",
      "episode: 125 reward: -127.943747590001\n",
      "episode: 126 reward: -132.8490115166216\n",
      "episode: 127 reward: -335.27356221215257\n",
      "episode: 128 reward: -373.7183361025064\n",
      "episode: 129 reward: -131.43544758674363\n",
      "episode: 130 reward: -245.06299434706972\n",
      "episode: 131 reward: -130.9612252328807\n",
      "episode: 132 reward: -248.52822280389654\n",
      "episode: 133 reward: -118.39797834919914\n",
      "episode: 134 reward: -3.9464689428235133\n",
      "episode: 135 reward: -231.29708643919264\n",
      "episode: 136 reward: -1.6724846813072467\n",
      "episode: 137 reward: -122.53690448171238\n",
      "episode: 138 reward: -132.72165488792282\n",
      "episode: 139 reward: -3.5316692765077047\n",
      "episode: 140 reward: -130.88653916480723\n",
      "episode: 141 reward: -365.40702954586703\n",
      "episode: 142 reward: -243.26182057035112\n",
      "episode: 143 reward: -239.8005532548827\n",
      "episode: 144 reward: -257.10545863646047\n",
      "episode: 145 reward: -3.3235144590082815\n",
      "episode: 146 reward: -249.50556174517564\n",
      "episode: 147 reward: -366.1496315966707\n",
      "episode: 148 reward: -128.84238943419672\n",
      "episode: 149 reward: -121.31388977044215\n",
      "episode: 150 reward: -131.55089383367792\n",
      "episode: 151 reward: -132.34878278089377\n",
      "episode: 152 reward: -258.6238099599049\n",
      "episode: 153 reward: -355.9758521788694\n",
      "episode: 154 reward: -248.79839579729924\n",
      "episode: 155 reward: -238.60532683174213\n",
      "episode: 156 reward: -250.2149994333355\n",
      "episode: 157 reward: -121.8299313427938\n",
      "episode: 158 reward: -124.96677202060884\n",
      "episode: 159 reward: -129.45120079608168\n",
      "episode: 160 reward: -368.7763296057372\n",
      "episode: 161 reward: -356.08092466724963\n",
      "episode: 162 reward: -2.176362296619704\n",
      "episode: 163 reward: -123.87004527478108\n",
      "episode: 164 reward: -243.9714900285058\n",
      "episode: 165 reward: -125.65561690147588\n",
      "episode: 166 reward: -368.5008230992417\n",
      "episode: 167 reward: -348.7014229749571\n",
      "episode: 168 reward: -130.11373629123239\n",
      "episode: 169 reward: -118.45720875814688\n",
      "episode: 170 reward: -562.3149181215083\n",
      "episode: 171 reward: -226.8406637082083\n",
      "episode: 172 reward: -248.82095253730654\n",
      "episode: 173 reward: -378.3188274536771\n",
      "episode: 174 reward: -131.96619674874466\n",
      "episode: 175 reward: -5.1673634199420455\n",
      "episode: 176 reward: -132.39272640024132\n",
      "episode: 177 reward: -4.841025262887274\n",
      "episode: 178 reward: -134.44685256935253\n",
      "episode: 179 reward: -229.9667372441191\n",
      "episode: 180 reward: -249.02946954637383\n",
      "episode: 181 reward: -239.94200403513113\n",
      "episode: 182 reward: -127.57870890426358\n",
      "episode: 183 reward: -122.14838522713276\n",
      "episode: 184 reward: -357.4861710272135\n",
      "episode: 185 reward: -380.33363608974895\n",
      "episode: 186 reward: -450.0153939000362\n",
      "episode: 187 reward: -134.5304139234361\n",
      "episode: 188 reward: -133.016832521332\n",
      "episode: 189 reward: -361.2232936588523\n",
      "episode: 190 reward: -2.0773850363221484\n",
      "episode: 191 reward: -247.00173675534577\n",
      "episode: 192 reward: -486.25959381418517\n",
      "episode: 193 reward: -2.6651829986028917\n",
      "episode: 194 reward: -1.873471749950223\n",
      "episode: 195 reward: -368.16067603268783\n",
      "episode: 196 reward: -245.16257408258878\n",
      "episode: 197 reward: -331.5524101141892\n",
      "episode: 198 reward: -348.785741556735\n",
      "episode: 199 reward: -261.4383384862546\n",
      "episode: 200 reward: -131.20610720048072\n",
      "episode: 201 reward: -345.8926192430949\n",
      "episode: 202 reward: -249.27772880085746\n",
      "episode: 203 reward: -126.0091006707731\n",
      "episode: 204 reward: -125.62273007294442\n",
      "episode: 205 reward: -2.469285437609625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 206 reward: -248.82777674744105\n",
      "episode: 207 reward: -3.054016114383303\n",
      "episode: 208 reward: -238.66160780389765\n",
      "episode: 209 reward: -248.050932900935\n",
      "episode: 210 reward: -353.25992282770585\n",
      "episode: 211 reward: -129.54628919548372\n",
      "episode: 212 reward: -4.54632948837826\n",
      "episode: 213 reward: -129.99118779889292\n",
      "episode: 214 reward: -340.40288375788873\n",
      "episode: 215 reward: -351.5844800320542\n",
      "episode: 216 reward: -352.69858158438035\n",
      "episode: 217 reward: -350.1055423158006\n",
      "episode: 218 reward: -223.10502159890706\n",
      "episode: 219 reward: -244.86279048064821\n",
      "episode: 220 reward: -136.30509715468872\n",
      "episode: 221 reward: -245.00085727168081\n",
      "episode: 222 reward: -127.99911661864036\n",
      "episode: 223 reward: -120.89792595129555\n",
      "episode: 224 reward: -135.07466241032287\n",
      "episode: 225 reward: -131.1610745848154\n",
      "episode: 226 reward: -368.0501637917551\n",
      "episode: 227 reward: -327.8714637202394\n",
      "episode: 228 reward: -134.0027744300492\n",
      "episode: 229 reward: -133.0268713987024\n",
      "episode: 230 reward: -125.39605895922016\n",
      "episode: 231 reward: -260.1283911770603\n",
      "episode: 232 reward: -132.89733530671515\n",
      "episode: 233 reward: -132.3065411331086\n",
      "episode: 234 reward: -351.2635929467569\n",
      "episode: 235 reward: -135.22710921803449\n",
      "episode: 236 reward: -248.94052069526072\n",
      "episode: 237 reward: -233.19017815079542\n",
      "episode: 238 reward: -361.2334765587218\n",
      "episode: 239 reward: -2.90239550737691\n",
      "episode: 240 reward: -247.87003264303735\n",
      "episode: 241 reward: -132.0088822535536\n",
      "episode: 242 reward: -133.80614913979005\n",
      "episode: 243 reward: -131.44572881122187\n",
      "episode: 244 reward: -1.4512691778679887\n",
      "episode: 245 reward: -128.0884755139871\n",
      "episode: 246 reward: -248.724519726282\n",
      "episode: 247 reward: -448.7620451340853\n",
      "episode: 248 reward: -120.32531288473812\n",
      "episode: 249 reward: -369.9982393464721\n",
      "\n",
      "(50000, 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "\n",
    "max_expert_num = 50000\n",
    "num_steps = 0\n",
    "expert_traj = []\n",
    "\n",
    "for i_episode in count():\n",
    "    ob = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        ac = ppo.get_action([ob])[0]\n",
    "        next_ob, reward, done, _ = env.step(ac)\n",
    "        ob = next_ob\n",
    "        total_reward += reward\n",
    "        expert_traj.append(np.hstack([ob, ac]))\n",
    "        num_steps += 1\n",
    "    \n",
    "    print(\"episode:\", i_episode, \"reward:\", total_reward)\n",
    "    \n",
    "    if num_steps >= max_expert_num:\n",
    "        break\n",
    "        \n",
    "expert_traj = np.stack(expert_traj)\n",
    "print()\n",
    "print(expert_traj.shape)\n",
    "print()\n",
    "np.save(\"expert_traj.npy\", expert_traj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
