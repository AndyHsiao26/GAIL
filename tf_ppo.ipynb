{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fyhsiao/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.distributions as dist\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create Environments</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from common.multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "num_envs = 16\n",
    "env_name = \"Pendulum-v0\"\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Neural Network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic:\n",
    "    def __init__(self, sess, obs, acs, hidden_size, name, trainable, init_std=1.0):\n",
    "        self.sess = sess\n",
    "        self.obs = obs\n",
    "        self.acs = acs\n",
    "        self.hidden_size = hidden_size\n",
    "        self.name = name\n",
    "        self.trainable = trainable\n",
    "        self.init_std = init_std\n",
    "\n",
    "        self.num_ac = self.acs.get_shape().as_list()[-1]\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            self._build_network()\n",
    "\n",
    "    def _build_network(self):\n",
    "        with tf.variable_scope('critic'):\n",
    "            c_h1 = layers.fully_connected(self.obs, self.hidden_size, trainable=self.trainable)\n",
    "            c_out = layers.fully_connected(c_h1, 1, activation_fn=None, trainable=self.trainable)\n",
    "\n",
    "        with tf.variable_scope('actor'):\n",
    "            a_h1 = layers.fully_connected(self.obs, self.hidden_size, trainable=self.trainable)\n",
    "            a_out = layers.fully_connected(a_h1, self.num_ac, activation_fn=None, trainable=self.trainable)\n",
    "\n",
    "            log_std = tf.get_variable('log_std', [1, self.num_ac], dtype=tf.float32,\n",
    "                                      initializer=tf.constant_initializer(self.init_std),\n",
    "                                      trainable=self.trainable)\n",
    "\n",
    "        std = tf.exp(log_std)\n",
    "        a_dist = dist.Normal(a_out, std)\n",
    "        self.log_prob = a_dist.log_prob(self.acs)\n",
    "        self.entropy = tf.reduce_mean(a_dist.entropy())\n",
    "\n",
    "        self.value = tf.identity(c_out)\n",
    "        self.action = a_dist.sample()\n",
    "\n",
    "    def params(self):\n",
    "        return tf.global_variables(self.name).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>GAE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Proximal Policy Optimization Algorithm</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, sess, ob_shape, ac_shape, lr, hidden_size, eps=0.2, v_coeff=0.5, ent_coeff=0.01):\n",
    "        self.sess = sess\n",
    "        self.ob_shape = ob_shape\n",
    "        self.ac_shape = ac_shape\n",
    "        self.lr = lr\n",
    "        self.hidden_size = hidden_size\n",
    "        self.eps = eps\n",
    "        self.v_coeff = v_coeff\n",
    "        self.ent_coeff = ent_coeff\n",
    "\n",
    "        self._create_ppo_graph()\n",
    "\n",
    "    def _create_ppo_graph(self):\n",
    "        self.obs = tf.placeholder(dtype=tf.float32, shape=[None] + self.ob_shape, name='observation')\n",
    "        self.acs = tf.placeholder(dtype=tf.float32, shape=[None] + self.ac_shape, name='action')\n",
    "        self.returns = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "        self.advs = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "\n",
    "        self.pi = ActorCritic(self.sess, self.obs, self.acs, self.hidden_size, 'new_pi', trainable=True)\n",
    "        self.old_pi = ActorCritic(self.sess, self.obs, self.acs, self.hidden_size, 'old_pi', trainable=False)\n",
    "\n",
    "        self.pi_param = self.pi.params()\n",
    "        self.old_pi_param = self.old_pi.params()\n",
    "\n",
    "        with tf.name_scope('update_old_policy'):\n",
    "            self.oldpi_update = [oldp.assign(p) for p, oldp in zip(self.pi_param, self.old_pi_param)]\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            ratio = tf.exp(self.pi.log_prob - self.old_pi.log_prob)\n",
    "            surr = ratio * self.advs\n",
    "            self.actor_loss = tf.reduce_mean(\n",
    "                tf.minimum(surr, tf.clip_by_value(ratio, 1 - self.eps, 1 + self.eps) * self.advs))\n",
    "            self.critic_loss = tf.reduce_mean(tf.square(self.returns - self.pi.value))\n",
    "\n",
    "            self.loss = (- self.actor_loss - self.ent_coeff * tf.reduce_mean(self.pi.entropy)\n",
    "                         + self.v_coeff * self.critic_loss)\n",
    "\n",
    "            with tf.variable_scope('train_op'):\n",
    "                grads = tf.gradients(self.loss, self.pi_param)\n",
    "                self.grads = list(zip(grads, self.pi_param))\n",
    "                self.train_op = tf.train.AdamOptimizer(self.lr).apply_gradients(self.grads)\n",
    "                                                                                #global_step=self.global_step)\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        return self.sess.run(self.pi.action, feed_dict={self.obs: obs})\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        return self.sess.run(self.pi.value, feed_dict={self.obs: obs})\n",
    "\n",
    "    def assign_old_pi(self):\n",
    "        self.sess.run(self.oldpi_update)\n",
    "\n",
    "    def update(self, obs, acs, returns, advs):\n",
    "        feed_dict = {self.obs: obs,\n",
    "                     self.acs: acs,\n",
    "                     self.returns: returns,\n",
    "                     self.advs: advs\n",
    "                     }\n",
    "\n",
    "        self.sess.run(self.train_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(mini_batch_size, obs, acs, returns, advantage):\n",
    "    batch_size = obs.shape[0]\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield (obs[rand_ids, :], acs[rand_ids, :],\n",
    "               returns[rand_ids, :], advantage[rand_ids, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Testing Function</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "def test_env(model, vis=False):\n",
    "    ob = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        if vis:\n",
    "            env.render()\n",
    "        ac = model.get_action([ob])[0]\n",
    "        next_ob, reward, done, _ = env.step(ac)\n",
    "        ob = next_ob\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hyperparameters</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "lr = 3e-4\n",
    "num_steps = 20\n",
    "mini_batch_size = 5\n",
    "ppo_epochs = 4\n",
    "threshold_reward = -200\n",
    "\n",
    "max_frames = 15000\n",
    "frame_idx  = 0\n",
    "test_rewards = []\n",
    "\n",
    "tf.set_random_seed(2018)\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd5440863c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ob_shape = list(envs.observation_space.shape)\n",
    "ac_shape = list(envs.action_space.shape)\n",
    "\n",
    "ob = envs.reset()\n",
    "early_stop = False\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "ppo = PPO(sess, ob_shape, ac_shape, lr, hidden_size)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "while frame_idx < max_frames and not early_stop:\n",
    "\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    obs = []\n",
    "    acs = []\n",
    "    rewards = []\n",
    "    masks = []\n",
    "    entropy = 0\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "\n",
    "        ac = ppo.get_action(ob)\n",
    "        next_ob, reward, done, _ = envs.step(ac)\n",
    "\n",
    "        value = ppo.get_value(ob)\n",
    "        values.append(value)\n",
    "        rewards.append(reward[:, np.newaxis])\n",
    "        masks.append((1-done)[:, np.newaxis])\n",
    "\n",
    "        obs.append(ob)\n",
    "        acs.append(ac)\n",
    "\n",
    "        ob = next_ob\n",
    "        frame_idx += 1\n",
    "\n",
    "        if frame_idx % 1000 == 0:\n",
    "            test_reward = np.mean([test_env(ppo) for _ in range(10)])\n",
    "            test_rewards.append(test_reward)\n",
    "            plot(frame_idx, test_rewards)\n",
    "            if test_reward > threshold_reward: early_stop = True\n",
    "\n",
    "    next_value = ppo.get_value(next_ob)\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "    returns = np.concatenate(returns)\n",
    "    values = np.concatenate(values)\n",
    "    obs = np.concatenate(obs)\n",
    "    acs = np.concatenate(acs)\n",
    "    advantages = returns - values\n",
    "\n",
    "    ppo.assign_old_pi()\n",
    "    for _ in range(ppo_epochs):\n",
    "        for ob_batch, ac_batch, return_batch, adv_batch in ppo_iter(mini_batch_size, obs, acs, returns, advantages):\n",
    "            ppo.update(ob_batch, ac_batch, return_batch, adv_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 reward: -2.0129301672606057\n",
      "episode: 1 reward: -1.285845970903421\n",
      "episode: 2 reward: -128.20041688857143\n",
      "episode: 3 reward: -131.21668432973308\n",
      "episode: 4 reward: -128.37844584407935\n",
      "episode: 5 reward: -237.51949373765265\n",
      "episode: 6 reward: -1.7271453493258935\n",
      "episode: 7 reward: -130.36405436514636\n",
      "episode: 8 reward: -126.02371938837105\n",
      "episode: 9 reward: -227.73706213177127\n",
      "episode: 10 reward: -1.3306064973921392\n",
      "episode: 11 reward: -127.15094725778006\n",
      "episode: 12 reward: -468.46664067030144\n",
      "episode: 13 reward: -228.84556594247408\n",
      "episode: 14 reward: -375.36039085062293\n",
      "episode: 15 reward: -234.0798020686252\n",
      "episode: 16 reward: -374.56040144210516\n",
      "episode: 17 reward: -127.0558234078564\n",
      "episode: 18 reward: -232.0590302235643\n",
      "episode: 19 reward: -3.368635571114377\n",
      "episode: 20 reward: -431.1286594341335\n",
      "episode: 21 reward: -247.58386519404723\n",
      "episode: 22 reward: -238.01146429084375\n",
      "episode: 23 reward: -442.7740047691038\n",
      "episode: 24 reward: -232.2303612667933\n",
      "episode: 25 reward: -127.7188239981959\n",
      "episode: 26 reward: -126.9551877499525\n",
      "episode: 27 reward: -127.16403036307524\n",
      "episode: 28 reward: -444.897053574869\n",
      "episode: 29 reward: -231.00885913563638\n",
      "episode: 30 reward: -1.8325473024832604\n",
      "episode: 31 reward: -129.1429312471106\n",
      "episode: 32 reward: -124.42328789367059\n",
      "episode: 33 reward: -231.34453325441413\n",
      "episode: 34 reward: -128.93471568486302\n",
      "episode: 35 reward: -130.5681351757427\n",
      "episode: 36 reward: -365.7565845943141\n",
      "episode: 37 reward: -236.62978606718562\n",
      "episode: 38 reward: -246.96517640697724\n",
      "episode: 39 reward: -124.77203079857436\n",
      "episode: 40 reward: -126.52721763351751\n",
      "episode: 41 reward: -2.2785130419353408\n",
      "episode: 42 reward: -122.70704326657108\n",
      "episode: 43 reward: -126.92910455689798\n",
      "episode: 44 reward: -224.19840632418098\n",
      "episode: 45 reward: -128.36135346215323\n",
      "episode: 46 reward: -119.36477128489419\n",
      "episode: 47 reward: -1.884456863400888\n",
      "episode: 48 reward: -367.4879162593079\n",
      "episode: 49 reward: -128.87931449401955\n",
      "episode: 50 reward: -122.68441498936247\n",
      "episode: 51 reward: -131.3626200053626\n",
      "episode: 52 reward: -131.07279693104516\n",
      "episode: 53 reward: -2.1431900384278353\n",
      "episode: 54 reward: -3.2993721926452477\n",
      "episode: 55 reward: -223.6800562873572\n",
      "episode: 56 reward: -229.40847433769008\n",
      "episode: 57 reward: -243.43557857799712\n",
      "episode: 58 reward: -128.60288791424153\n",
      "episode: 59 reward: -352.5544455647155\n",
      "episode: 60 reward: -129.5448629068893\n",
      "episode: 61 reward: -228.27878627238323\n",
      "episode: 62 reward: -2.857497952829732\n",
      "episode: 63 reward: -230.00643876247398\n",
      "episode: 64 reward: -1.41751213758906\n",
      "episode: 65 reward: -222.12986346359014\n",
      "episode: 66 reward: -371.6770125737303\n",
      "episode: 67 reward: -120.30865228789257\n",
      "episode: 68 reward: -233.16243544231517\n",
      "episode: 69 reward: -1.6854282346254545\n",
      "episode: 70 reward: -429.9281920377699\n",
      "episode: 71 reward: -236.53266008698924\n",
      "episode: 72 reward: -119.2448003578754\n",
      "episode: 73 reward: -128.66171435279026\n",
      "episode: 74 reward: -240.39598665152354\n",
      "episode: 75 reward: -244.8720170611765\n",
      "episode: 76 reward: -125.02544325565687\n",
      "episode: 77 reward: -121.96299472482569\n",
      "episode: 78 reward: -1.5023541969867706\n",
      "episode: 79 reward: -471.65174402296714\n",
      "episode: 80 reward: -237.78921581675596\n",
      "episode: 81 reward: -120.75737426413201\n",
      "episode: 82 reward: -365.76830838728\n",
      "episode: 83 reward: -244.7861679820428\n",
      "episode: 84 reward: -118.80236314795403\n",
      "episode: 85 reward: -249.97791786847145\n",
      "episode: 86 reward: -352.7752149650836\n",
      "episode: 87 reward: -2.913669275981597\n",
      "episode: 88 reward: -121.43078536803415\n",
      "episode: 89 reward: -131.86927365586837\n",
      "episode: 90 reward: -114.383067709132\n",
      "episode: 91 reward: -116.72476408441982\n",
      "episode: 92 reward: -131.1452388688564\n",
      "episode: 93 reward: -125.6174973615484\n",
      "episode: 94 reward: -231.33840179205805\n",
      "episode: 95 reward: -241.49459874509995\n",
      "episode: 96 reward: -128.43256335568256\n",
      "episode: 97 reward: -1.606728470951637\n",
      "episode: 98 reward: -340.08931627224933\n",
      "episode: 99 reward: -114.07706276074204\n",
      "episode: 100 reward: -123.0346817338264\n",
      "episode: 101 reward: -463.299297808567\n",
      "episode: 102 reward: -232.8527383507599\n",
      "episode: 103 reward: -128.1451283651575\n",
      "episode: 104 reward: -232.2201149355055\n",
      "episode: 105 reward: -127.09105913206254\n",
      "episode: 106 reward: -131.9856468558414\n",
      "episode: 107 reward: -123.80166113917376\n",
      "episode: 108 reward: -462.70926959331075\n",
      "episode: 109 reward: -244.29157318090182\n",
      "episode: 110 reward: -126.14715202661047\n",
      "episode: 111 reward: -2.663777875676551\n",
      "episode: 112 reward: -116.91973530694536\n",
      "episode: 113 reward: -3.3608713541158104\n",
      "episode: 114 reward: -261.008863768426\n",
      "episode: 115 reward: -1.589826255614377\n",
      "episode: 116 reward: -129.237343655476\n",
      "episode: 117 reward: -254.50673070681012\n",
      "episode: 118 reward: -130.79230868573993\n",
      "episode: 119 reward: -236.06833282104296\n",
      "episode: 120 reward: -126.93526930577205\n",
      "episode: 121 reward: -238.67645458107017\n",
      "episode: 122 reward: -124.52657636648438\n",
      "episode: 123 reward: -215.79069203555824\n",
      "episode: 124 reward: -132.53524153219948\n",
      "episode: 125 reward: -417.2857339373163\n",
      "episode: 126 reward: -118.90114062067444\n",
      "episode: 127 reward: -239.48109899816663\n",
      "episode: 128 reward: -2.3578294854437276\n",
      "episode: 129 reward: -230.48090658013237\n",
      "episode: 130 reward: -224.27678007066064\n",
      "episode: 131 reward: -245.63829014551\n",
      "episode: 132 reward: -244.56626545128495\n",
      "episode: 133 reward: -224.80499347253178\n",
      "episode: 134 reward: -127.7321587705102\n",
      "episode: 135 reward: -130.46482177783372\n",
      "episode: 136 reward: -128.72086852933685\n",
      "episode: 137 reward: -129.42838258053294\n",
      "episode: 138 reward: -127.07192653761824\n",
      "episode: 139 reward: -238.97825278255291\n",
      "episode: 140 reward: -445.93399402075113\n",
      "episode: 141 reward: -131.59493085320761\n",
      "episode: 142 reward: -249.8247499459263\n",
      "episode: 143 reward: -254.95274893413065\n",
      "episode: 144 reward: -1.358296332903801\n",
      "episode: 145 reward: -128.46493699905633\n",
      "episode: 146 reward: -128.02910774713135\n",
      "episode: 147 reward: -440.0980156696374\n",
      "episode: 148 reward: -1.054996771892734\n",
      "episode: 149 reward: -235.92097139448452\n",
      "episode: 150 reward: -251.47589644294507\n",
      "episode: 151 reward: -1.2249603718999387\n",
      "episode: 152 reward: -1.5964173947671838\n",
      "episode: 153 reward: -1.2110329870148049\n",
      "episode: 154 reward: -321.895920863243\n",
      "episode: 155 reward: -222.9982930502545\n",
      "episode: 156 reward: -121.1964726827224\n",
      "episode: 157 reward: -125.20266657578564\n",
      "episode: 158 reward: -123.31640281305043\n",
      "episode: 159 reward: -496.5882490281819\n",
      "episode: 160 reward: -510.17292371512724\n",
      "episode: 161 reward: -239.918279515378\n",
      "episode: 162 reward: -352.4509061559414\n",
      "episode: 163 reward: -241.5181457390392\n",
      "episode: 164 reward: -129.02543444381382\n",
      "episode: 165 reward: -245.8714106710863\n",
      "episode: 166 reward: -506.7300485997101\n",
      "episode: 167 reward: -430.38715629809894\n",
      "episode: 168 reward: -247.13042124201226\n",
      "episode: 169 reward: -124.76288587238521\n",
      "episode: 170 reward: -232.94845278880732\n",
      "episode: 171 reward: -130.38863699410348\n",
      "episode: 172 reward: -128.74773596066788\n",
      "episode: 173 reward: -479.99884503617704\n",
      "episode: 174 reward: -1.1554293413168888\n",
      "episode: 175 reward: -484.067901982856\n",
      "episode: 176 reward: -131.7118265054396\n",
      "episode: 177 reward: -224.84806314196234\n",
      "episode: 178 reward: -258.39450836672864\n",
      "episode: 179 reward: -126.04440038803128\n",
      "episode: 180 reward: -124.58914072357489\n",
      "episode: 181 reward: -354.5222322334864\n",
      "episode: 182 reward: -385.0811379482797\n",
      "episode: 183 reward: -122.23380145506057\n",
      "episode: 184 reward: -238.0415149134456\n",
      "episode: 185 reward: -238.33442647361477\n",
      "episode: 186 reward: -129.60378285658078\n",
      "episode: 187 reward: -374.9401635714941\n",
      "episode: 188 reward: -343.9997928150069\n",
      "episode: 189 reward: -232.2984173886456\n",
      "episode: 190 reward: -121.18764919843079\n",
      "episode: 191 reward: -334.3022939100582\n",
      "episode: 192 reward: -126.98463005182629\n",
      "episode: 193 reward: -1.79648183251261\n",
      "episode: 194 reward: -238.7479407316758\n",
      "episode: 195 reward: -233.02451781619285\n",
      "episode: 196 reward: -3.4019318368573708\n",
      "episode: 197 reward: -226.2845099855062\n",
      "episode: 198 reward: -225.77423495559245\n",
      "episode: 199 reward: -126.87007254361542\n",
      "episode: 200 reward: -251.2088072057253\n",
      "episode: 201 reward: -2.0441128094785683\n",
      "episode: 202 reward: -251.40879926673296\n",
      "episode: 203 reward: -239.24933657723057\n",
      "episode: 204 reward: -350.7254359045731\n",
      "episode: 205 reward: -131.5118106923686\n",
      "episode: 206 reward: -240.88834432741237\n",
      "episode: 207 reward: -2.0716788258063614\n",
      "episode: 208 reward: -253.28964870072568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 209 reward: -246.9260135756754\n",
      "episode: 210 reward: -120.8627230966021\n",
      "episode: 211 reward: -359.9465644768023\n",
      "episode: 212 reward: -128.8046213027746\n",
      "episode: 213 reward: -120.86487849138263\n",
      "episode: 214 reward: -226.2032438701778\n",
      "episode: 215 reward: -249.5000400691025\n",
      "episode: 216 reward: -118.45093360550433\n",
      "episode: 217 reward: -363.06498428706203\n",
      "episode: 218 reward: -1.369335050250744\n",
      "episode: 219 reward: -241.2773374968173\n",
      "episode: 220 reward: -225.27245518847784\n",
      "episode: 221 reward: -356.57426444053965\n",
      "episode: 222 reward: -117.45418132471212\n",
      "episode: 223 reward: -4.335988078696339\n",
      "episode: 224 reward: -373.1159202814393\n",
      "episode: 225 reward: -125.83001627756163\n",
      "episode: 226 reward: -226.70645184231753\n",
      "episode: 227 reward: -130.97441847914172\n",
      "episode: 228 reward: -0.975800475100304\n",
      "episode: 229 reward: -117.96284615517395\n",
      "episode: 230 reward: -340.9223286590126\n",
      "episode: 231 reward: -126.30858489337203\n",
      "episode: 232 reward: -123.36203883920943\n",
      "episode: 233 reward: -130.40131421422254\n",
      "episode: 234 reward: -125.01152581134734\n",
      "episode: 235 reward: -239.3458953084089\n",
      "episode: 236 reward: -127.41924701176524\n",
      "episode: 237 reward: -223.8858294123204\n",
      "episode: 238 reward: -240.7582055120043\n",
      "episode: 239 reward: -348.0096544377743\n",
      "episode: 240 reward: -232.04539657878962\n",
      "episode: 241 reward: -312.97706273474415\n",
      "episode: 242 reward: -234.47741634186946\n",
      "episode: 243 reward: -123.79050444042974\n",
      "episode: 244 reward: -472.64875246935964\n",
      "episode: 245 reward: -130.22719416360917\n",
      "episode: 246 reward: -1.536717993462841\n",
      "episode: 247 reward: -238.00417118571818\n",
      "episode: 248 reward: -238.29582296927737\n",
      "episode: 249 reward: -121.62278929106253\n",
      "\n",
      "(50000, 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "\n",
    "max_expert_num = 50000\n",
    "num_steps = 0\n",
    "expert_traj = []\n",
    "\n",
    "for i_episode in count():\n",
    "    ob = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        ac = ppo.get_action([ob])[0]\n",
    "        next_ob, reward, done, _ = env.step(ac)\n",
    "        ob = next_ob\n",
    "        total_reward += reward\n",
    "        expert_traj.append(np.hstack([ob, ac]))\n",
    "        num_steps += 1\n",
    "    \n",
    "    print(\"episode:\", i_episode, \"reward:\", total_reward)\n",
    "    \n",
    "    if num_steps >= max_expert_num:\n",
    "        break\n",
    "        \n",
    "expert_traj = np.stack(expert_traj)\n",
    "print()\n",
    "print(expert_traj.shape)\n",
    "print()\n",
    "np.save(\"expert_traj.npy\", expert_traj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
