{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fyhsiao/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.distributions as dist\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "tf.set_random_seed(2019)\n",
    "np.random.seed(2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create Environments</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from common.multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "num_envs = 16\n",
    "env_name = \"Pendulum-v0\"\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Neural Network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic:\n",
    "    def __init__(self, sess, obs, acs, hidden_size, name, trainable, init_std=1.0):\n",
    "        self.sess = sess\n",
    "        self.obs = obs\n",
    "        self.acs = acs\n",
    "        self.hidden_size = hidden_size\n",
    "        self.name = name\n",
    "        self.trainable = trainable\n",
    "        self.init_std = init_std\n",
    "\n",
    "        self.num_ac = self.acs.get_shape().as_list()[-1]\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            self._build_network()\n",
    "\n",
    "    def _build_network(self):\n",
    "        with tf.variable_scope('critic'):\n",
    "            c_h1 = layers.fully_connected(self.obs, self.hidden_size, trainable=self.trainable)\n",
    "            c_out = layers.fully_connected(c_h1, 1, activation_fn=None, trainable=self.trainable)\n",
    "\n",
    "        with tf.variable_scope('actor'):\n",
    "            a_h1 = layers.fully_connected(self.obs, self.hidden_size, trainable=self.trainable)\n",
    "            a_out = layers.fully_connected(a_h1, self.num_ac, activation_fn=None, trainable=self.trainable)\n",
    "\n",
    "            log_std = tf.get_variable('log_std', [1, self.num_ac], dtype=tf.float32,\n",
    "                                      initializer=tf.constant_initializer(self.init_std),\n",
    "                                      trainable=self.trainable)\n",
    "\n",
    "        std = tf.exp(log_std)\n",
    "        a_dist = dist.Normal(a_out, std)\n",
    "        self.log_prob = a_dist.log_prob(self.acs)\n",
    "        self.entropy = tf.reduce_mean(a_dist.entropy())\n",
    "\n",
    "        self.value = tf.identity(c_out)\n",
    "        self.action = a_dist.sample()\n",
    "\n",
    "    def params(self):\n",
    "        return tf.global_variables(self.name).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>GAE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Proximal Policy Optimization Algorithm</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, sess, ob_shape, ac_shape, lr, hidden_size, eps=0.2, v_coeff=0.5, ent_coeff=0.01):\n",
    "        self.sess = sess\n",
    "        self.ob_shape = ob_shape\n",
    "        self.ac_shape = ac_shape\n",
    "        self.lr = lr\n",
    "        self.hidden_size = hidden_size\n",
    "        self.eps = eps\n",
    "        self.v_coeff = v_coeff\n",
    "        self.ent_coeff = ent_coeff\n",
    "\n",
    "        self._create_ppo_graph()\n",
    "\n",
    "    def _create_ppo_graph(self):\n",
    "        self.obs = tf.placeholder(dtype=tf.float32, shape=[None] + self.ob_shape, name='observation')\n",
    "        self.acs = tf.placeholder(dtype=tf.float32, shape=[None] + self.ac_shape, name='action')\n",
    "        self.returns = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "        self.advs = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "\n",
    "        self.pi = ActorCritic(self.sess, self.obs, self.acs, self.hidden_size, 'new_pi', trainable=True)\n",
    "        self.old_pi = ActorCritic(self.sess, self.obs, self.acs, self.hidden_size, 'old_pi', trainable=False)\n",
    "\n",
    "        self.pi_param = self.pi.params()\n",
    "        self.old_pi_param = self.old_pi.params()\n",
    "\n",
    "        with tf.name_scope('update_old_policy'):\n",
    "            self.oldpi_update = [oldp.assign(p) for p, oldp in zip(self.pi_param, self.old_pi_param)]\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            ratio = tf.exp(self.pi.log_prob - self.old_pi.log_prob)\n",
    "            surr = ratio * self.advs\n",
    "            self.actor_loss = tf.reduce_mean(\n",
    "                tf.minimum(surr, tf.clip_by_value(ratio, 1 - self.eps, 1 + self.eps) * self.advs))\n",
    "            self.critic_loss = tf.reduce_mean(tf.square(self.returns - self.pi.value))\n",
    "\n",
    "            self.loss = (- self.actor_loss - self.ent_coeff * tf.reduce_mean(self.pi.entropy)\n",
    "                         + self.v_coeff * self.critic_loss)\n",
    "\n",
    "            with tf.variable_scope('train_op'):\n",
    "                grads = tf.gradients(self.loss, self.pi_param)\n",
    "                self.grads = list(zip(grads, self.pi_param))\n",
    "                self.train_op = tf.train.AdamOptimizer(self.lr).apply_gradients(self.grads)\n",
    "                                                                                #global_step=self.global_step)\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        return self.sess.run(self.pi.action, feed_dict={self.obs: obs})\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        return self.sess.run(self.pi.value, feed_dict={self.obs: obs})\n",
    "\n",
    "    def assign_old_pi(self):\n",
    "        self.sess.run(self.oldpi_update)\n",
    "\n",
    "    def update(self, obs, acs, returns, advs):\n",
    "        feed_dict = {self.obs: obs,\n",
    "                     self.acs: acs,\n",
    "                     self.returns: returns,\n",
    "                     self.advs: advs\n",
    "                     }\n",
    "\n",
    "        self.sess.run(self.train_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(mini_batch_size, obs, acs, returns, advantage):\n",
    "    batch_size = obs.shape[0]\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield (obs[rand_ids, :], acs[rand_ids, :],\n",
    "               returns[rand_ids, :], advantage[rand_ids, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Testing Function</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "def test_env(model, vis=False):\n",
    "    ob = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        if vis:\n",
    "            env.render()\n",
    "        ac = model.get_action([ob])[0]\n",
    "        next_ob, reward, done, _ = env.step(ac)\n",
    "        ob = next_ob\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hyperparameters</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "lr = 3e-4\n",
    "num_steps = 20\n",
    "mini_batch_size = 5\n",
    "ppo_epochs = 4\n",
    "threshold_reward = -200\n",
    "\n",
    "max_frames = 15000\n",
    "frame_idx  = 0\n",
    "test_rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f56847733c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ob_shape = list(envs.observation_space.shape)\n",
    "ac_shape = list(envs.action_space.shape)\n",
    "\n",
    "ob = envs.reset()\n",
    "early_stop = False\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "ppo = PPO(sess, ob_shape, ac_shape, lr, hidden_size)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "while frame_idx < max_frames and not early_stop:\n",
    "\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    obs = []\n",
    "    acs = []\n",
    "    rewards = []\n",
    "    masks = []\n",
    "    entropy = 0\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "\n",
    "        ac = ppo.get_action(ob)\n",
    "        next_ob, reward, done, _ = envs.step(ac)\n",
    "\n",
    "        value = ppo.get_value(ob)\n",
    "        values.append(value)\n",
    "        rewards.append(reward[:, np.newaxis])\n",
    "        masks.append((1-done)[:, np.newaxis])\n",
    "\n",
    "        obs.append(ob)\n",
    "        acs.append(ac)\n",
    "\n",
    "        ob = next_ob\n",
    "        frame_idx += 1\n",
    "\n",
    "        if frame_idx % 1000 == 0:\n",
    "            test_reward = np.mean([test_env(ppo) for _ in range(10)])\n",
    "            test_rewards.append(test_reward)\n",
    "            plot(frame_idx, test_rewards)\n",
    "            if test_reward > threshold_reward: early_stop = True\n",
    "\n",
    "    next_value = ppo.get_value(next_ob)\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "    returns = np.concatenate(returns)\n",
    "    values = np.concatenate(values)\n",
    "    obs = np.concatenate(obs)\n",
    "    acs = np.concatenate(acs)\n",
    "    advantages = returns - values\n",
    "\n",
    "    ppo.assign_old_pi()\n",
    "    for _ in range(ppo_epochs):\n",
    "        for ob_batch, ac_batch, return_batch, adv_batch in ppo_iter(mini_batch_size, obs, acs, returns, advantages):\n",
    "            ppo.update(ob_batch, ac_batch, return_batch, adv_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 reward: -251.1548507309005\n",
      "episode: 1 reward: -132.37287041964498\n",
      "episode: 2 reward: -130.5941144424003\n",
      "episode: 3 reward: -261.23551397775327\n",
      "episode: 4 reward: -915.5105410670036\n",
      "episode: 5 reward: -242.90445628826066\n",
      "episode: 6 reward: -654.1614463788063\n",
      "episode: 7 reward: -121.06892706846007\n",
      "episode: 8 reward: -244.84919156687877\n",
      "episode: 9 reward: -130.0215454711023\n",
      "episode: 10 reward: -250.80028266131043\n",
      "episode: 11 reward: -393.2755174293997\n",
      "episode: 12 reward: -656.2668391616673\n",
      "episode: 13 reward: -398.21076667125675\n",
      "episode: 14 reward: -931.3127651374143\n",
      "episode: 15 reward: -377.6169966729611\n",
      "episode: 16 reward: -3.329984183301403\n",
      "episode: 17 reward: -123.41143239084602\n",
      "episode: 18 reward: -416.11507408279016\n",
      "episode: 19 reward: -132.48519859520022\n",
      "episode: 20 reward: -529.4471678090156\n",
      "episode: 21 reward: -134.66440666908773\n",
      "episode: 22 reward: -129.91533046400838\n",
      "episode: 23 reward: -128.47863378081277\n",
      "episode: 24 reward: -250.65963433451464\n",
      "episode: 25 reward: -402.69053149436985\n",
      "episode: 26 reward: -136.28002255003838\n",
      "episode: 27 reward: -130.38974931020115\n",
      "episode: 28 reward: -132.69130937958965\n",
      "episode: 29 reward: -456.79714966007236\n",
      "episode: 30 reward: -368.4148446700339\n",
      "episode: 31 reward: -250.2647245076122\n",
      "episode: 32 reward: -2.5914157680193397\n",
      "episode: 33 reward: -245.96208458985964\n",
      "episode: 34 reward: -127.17052959547793\n",
      "episode: 35 reward: -280.5140754018481\n",
      "episode: 36 reward: -256.67149779155903\n",
      "episode: 37 reward: -263.52080631507687\n",
      "episode: 38 reward: -811.9638833251337\n",
      "episode: 39 reward: -4.205941012364166\n",
      "episode: 40 reward: -3.6212857067374467\n",
      "episode: 41 reward: -1.8415876907069249\n",
      "episode: 42 reward: -786.1938414371613\n",
      "episode: 43 reward: -132.21470167325725\n",
      "episode: 44 reward: -129.44546175977007\n",
      "episode: 45 reward: -256.1373795142738\n",
      "episode: 46 reward: -131.8491770724321\n",
      "episode: 47 reward: -265.6004562562716\n",
      "episode: 48 reward: -263.88737942877077\n",
      "episode: 49 reward: -355.65075134793875\n",
      "episode: 50 reward: -126.32277673481128\n",
      "episode: 51 reward: -256.2350466860874\n",
      "episode: 52 reward: -134.3449811890127\n",
      "episode: 53 reward: -132.0902590761142\n",
      "episode: 54 reward: -124.96974494904546\n",
      "episode: 55 reward: -264.15664793041714\n",
      "episode: 56 reward: -134.02995825832832\n",
      "episode: 57 reward: -133.9800571963082\n",
      "episode: 58 reward: -259.73629566886757\n",
      "episode: 59 reward: -262.7079994724372\n",
      "episode: 60 reward: -256.2028591519636\n",
      "episode: 61 reward: -126.11723857551452\n",
      "episode: 62 reward: -126.18808952825522\n",
      "episode: 63 reward: -474.53312764270936\n",
      "episode: 64 reward: -130.89192130157247\n",
      "episode: 65 reward: -131.28308933326724\n",
      "episode: 66 reward: -269.58702033513987\n",
      "episode: 67 reward: -267.02781981833164\n",
      "episode: 68 reward: -128.28442898696386\n",
      "episode: 69 reward: -389.49648443639956\n",
      "episode: 70 reward: -9.738538424928308\n",
      "episode: 71 reward: -261.6378224090895\n",
      "episode: 72 reward: -130.0502765912976\n",
      "episode: 73 reward: -249.65434517468216\n",
      "episode: 74 reward: -528.2436290700509\n",
      "episode: 75 reward: -251.59227661878126\n",
      "episode: 76 reward: -415.925195345948\n",
      "episode: 77 reward: -238.10693400693634\n",
      "episode: 78 reward: -242.2352486717627\n",
      "episode: 79 reward: -249.43712153169736\n",
      "episode: 80 reward: -239.67435176962496\n",
      "episode: 81 reward: -386.1541380941022\n",
      "episode: 82 reward: -265.6226377064236\n",
      "episode: 83 reward: -130.99661449724954\n",
      "episode: 84 reward: -126.87855366605329\n",
      "episode: 85 reward: -259.04435949009905\n",
      "episode: 86 reward: -132.66934949401306\n",
      "episode: 87 reward: -789.0413304913554\n",
      "episode: 88 reward: -358.3379073612721\n",
      "episode: 89 reward: -132.60315695199662\n",
      "episode: 90 reward: -6.797052960912348\n",
      "episode: 91 reward: -132.1210246238964\n",
      "episode: 92 reward: -263.8937739513575\n",
      "episode: 93 reward: -236.59988062194867\n",
      "episode: 94 reward: -256.28834854753194\n",
      "episode: 95 reward: -128.68254456587107\n",
      "episode: 96 reward: -131.82361048050427\n",
      "episode: 97 reward: -131.1315423934977\n",
      "episode: 98 reward: -131.94852973758634\n",
      "episode: 99 reward: -4.930161532894669\n",
      "episode: 100 reward: -377.2920944816126\n",
      "episode: 101 reward: -262.635914917952\n",
      "episode: 102 reward: -3.813931353524581\n",
      "episode: 103 reward: -138.5332197199498\n",
      "episode: 104 reward: -127.50851244031055\n",
      "episode: 105 reward: -133.04159655195247\n",
      "episode: 106 reward: -131.69973925832085\n",
      "episode: 107 reward: -130.42721435500894\n",
      "episode: 108 reward: -924.5696256132414\n",
      "episode: 109 reward: -130.30384122860596\n",
      "episode: 110 reward: -250.48560056927764\n",
      "episode: 111 reward: -354.3110995100044\n",
      "episode: 112 reward: -261.6179581551707\n",
      "episode: 113 reward: -133.6540989874535\n",
      "episode: 114 reward: -3.5755055755355625\n",
      "episode: 115 reward: -131.40670347128253\n",
      "episode: 116 reward: -258.3133624136317\n",
      "episode: 117 reward: -3.9548795551352374\n",
      "episode: 118 reward: -248.46684277079027\n",
      "episode: 119 reward: -131.3856591956748\n",
      "episode: 120 reward: -394.28335053126796\n",
      "episode: 121 reward: -377.864390550984\n",
      "episode: 122 reward: -250.51696010498844\n",
      "episode: 123 reward: -127.6435491824764\n",
      "episode: 124 reward: -256.3486198678445\n",
      "episode: 125 reward: -260.2411036306771\n",
      "episode: 126 reward: -130.96371053783415\n",
      "episode: 127 reward: -128.59611616612003\n",
      "episode: 128 reward: -372.8635694647983\n",
      "episode: 129 reward: -129.47150171566076\n",
      "episode: 130 reward: -260.5866049573938\n",
      "episode: 131 reward: -3.6843999742059736\n",
      "episode: 132 reward: -364.8054449246627\n",
      "episode: 133 reward: -133.68310676579023\n",
      "episode: 134 reward: -260.4345461337567\n",
      "episode: 135 reward: -377.8060910195609\n",
      "episode: 136 reward: -473.5561113762881\n",
      "episode: 137 reward: -252.21480271104568\n",
      "episode: 138 reward: -390.30326785046606\n",
      "episode: 139 reward: -366.3190041003733\n",
      "episode: 140 reward: -799.4034842789806\n",
      "episode: 141 reward: -134.39568247758106\n",
      "episode: 142 reward: -265.1225278148862\n",
      "episode: 143 reward: -156.98696311863307\n",
      "episode: 144 reward: -394.79687309040264\n",
      "episode: 145 reward: -246.87766175348088\n",
      "episode: 146 reward: -6.801124104738842\n",
      "episode: 147 reward: -260.57283547530585\n",
      "episode: 148 reward: -365.2512883683578\n",
      "episode: 149 reward: -130.2575403244573\n",
      "episode: 150 reward: -946.9335908752394\n",
      "episode: 151 reward: -3.9403152999388364\n",
      "episode: 152 reward: -130.4732398974635\n",
      "episode: 153 reward: -130.29132500735463\n",
      "episode: 154 reward: -131.29257733315123\n",
      "episode: 155 reward: -262.593900269117\n",
      "episode: 156 reward: -131.4147053890918\n",
      "episode: 157 reward: -314.27601829204855\n",
      "episode: 158 reward: -4.25277919165218\n",
      "episode: 159 reward: -135.7668509324046\n",
      "episode: 160 reward: -256.7347998236805\n",
      "episode: 161 reward: -254.14277276771628\n",
      "episode: 162 reward: -134.12352678585086\n",
      "episode: 163 reward: -255.5738066309417\n",
      "episode: 164 reward: -134.04872395699988\n",
      "episode: 165 reward: -398.1344433628272\n",
      "episode: 166 reward: -261.8176291545687\n",
      "episode: 167 reward: -130.04439690560895\n",
      "episode: 168 reward: -262.82873798163473\n",
      "episode: 169 reward: -385.2644957669773\n",
      "episode: 170 reward: -235.21147010021537\n",
      "episode: 171 reward: -525.3403366239688\n",
      "episode: 172 reward: -128.3247040555457\n",
      "episode: 173 reward: -131.85604678602715\n",
      "episode: 174 reward: -420.69661568746756\n",
      "episode: 175 reward: -131.0514339645269\n",
      "episode: 176 reward: -395.552146226852\n",
      "episode: 177 reward: -126.58340536149608\n",
      "episode: 178 reward: -134.31539145271148\n",
      "episode: 179 reward: -248.1154814002928\n",
      "episode: 180 reward: -962.0163203312195\n",
      "episode: 181 reward: -261.5033761937053\n",
      "episode: 182 reward: -369.7877928140796\n",
      "episode: 183 reward: -138.13144565897156\n",
      "episode: 184 reward: -136.7522280032193\n",
      "episode: 185 reward: -122.12515935234399\n",
      "episode: 186 reward: -130.6063845536299\n",
      "episode: 187 reward: -263.21910190067547\n",
      "episode: 188 reward: -262.607029321096\n",
      "episode: 189 reward: -260.7172313009502\n",
      "episode: 190 reward: -129.55408805728322\n",
      "episode: 191 reward: -257.40138983661467\n",
      "episode: 192 reward: -371.07743302480975\n",
      "episode: 193 reward: -6.113248872649503\n",
      "episode: 194 reward: -238.0037593009436\n",
      "episode: 195 reward: -132.5400725901432\n",
      "episode: 196 reward: -261.5418075005795\n",
      "episode: 197 reward: -262.29571308337177\n",
      "episode: 198 reward: -511.5405330974092\n",
      "episode: 199 reward: -641.2968825054377\n",
      "episode: 200 reward: -265.8886291413724\n",
      "episode: 201 reward: -129.6140549826575\n",
      "episode: 202 reward: -125.87967335980463\n",
      "episode: 203 reward: -129.78690056773718\n",
      "episode: 204 reward: -655.681313572618\n",
      "episode: 205 reward: -130.8225735888953\n",
      "episode: 206 reward: -252.81693645385738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 207 reward: -3.6902265762474005\n",
      "episode: 208 reward: -133.34465145373036\n",
      "episode: 209 reward: -248.66093578864871\n",
      "episode: 210 reward: -523.3571429133189\n",
      "episode: 211 reward: -355.0820012348611\n",
      "episode: 212 reward: -265.71514976400346\n",
      "episode: 213 reward: -264.88618344847555\n",
      "episode: 214 reward: -123.40894914154217\n",
      "episode: 215 reward: -260.0412229088417\n",
      "episode: 216 reward: -4.142518081435341\n",
      "episode: 217 reward: -127.95175376907716\n",
      "episode: 218 reward: -790.0029019311495\n",
      "episode: 219 reward: -255.7143431421128\n",
      "episode: 220 reward: -252.4985524674793\n",
      "episode: 221 reward: -132.48406298111107\n",
      "episode: 222 reward: -134.1416311505178\n",
      "episode: 223 reward: -130.22448791463154\n",
      "episode: 224 reward: -4.473883571978505\n",
      "episode: 225 reward: -129.70651946543376\n",
      "episode: 226 reward: -259.3950594369899\n",
      "episode: 227 reward: -258.90379803641747\n",
      "episode: 228 reward: -254.8528165503973\n",
      "episode: 229 reward: -129.2124071513761\n",
      "episode: 230 reward: -130.1209083043378\n",
      "episode: 231 reward: -120.65751436017972\n",
      "episode: 232 reward: -382.0081981580766\n",
      "episode: 233 reward: -261.79125904276935\n",
      "episode: 234 reward: -132.42738206075708\n",
      "episode: 235 reward: -921.2497367865246\n",
      "episode: 236 reward: -127.67800197896128\n",
      "episode: 237 reward: -253.0950033605083\n",
      "episode: 238 reward: -265.63576909993867\n",
      "episode: 239 reward: -400.1229594106687\n",
      "episode: 240 reward: -133.1473153683106\n",
      "episode: 241 reward: -133.50981944587053\n",
      "episode: 242 reward: -127.74667995110491\n",
      "episode: 243 reward: -133.17236213192163\n",
      "episode: 244 reward: -125.00895864617327\n",
      "episode: 245 reward: -484.90746976687336\n",
      "episode: 246 reward: -258.90341787585527\n",
      "episode: 247 reward: -359.0432087208852\n",
      "episode: 248 reward: -783.4271797781901\n",
      "episode: 249 reward: -253.08922792921902\n",
      "\n",
      "(50000, 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "\n",
    "max_expert_num = 50000\n",
    "num_steps = 0\n",
    "expert_traj = []\n",
    "\n",
    "for i_episode in count():\n",
    "    ob = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        ac = ppo.get_action([ob])[0]\n",
    "        next_ob, reward, done, _ = env.step(ac)\n",
    "        ob = next_ob\n",
    "        total_reward += reward\n",
    "        expert_traj.append(np.hstack([ob, ac]))\n",
    "        num_steps += 1\n",
    "    \n",
    "    print(\"episode:\", i_episode, \"reward:\", total_reward)\n",
    "    \n",
    "    if num_steps >= max_expert_num:\n",
    "        break\n",
    "        \n",
    "expert_traj = np.stack(expert_traj)\n",
    "print()\n",
    "print(expert_traj.shape)\n",
    "print()\n",
    "np.save(\"expert_traj.npy\", expert_traj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
